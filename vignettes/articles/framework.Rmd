---
title: 'StreamFind General Introduction'
author:
  name: Ricardo Cunha
  email: cunha@iuta.de
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  bookdown::html_document2:
    fig_caption: true
    toc: true
    number_sections: true
    toc_float: true
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{StreamFind General Introduction}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
# Libraries
library(knitr)
library(kableExtra)
library(magrittr)
library(ggplot2)
library(plotly)
library(StreamFind)

# Global options
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", fig.width = 9, results = "markup", comment = '', message = FALSE, warning = FALSE)

# Working directory
path_wd <- getwd()

```

```{r resources, include=FALSE}
# MS files
all_files <- StreamFindData::get_ms_file_paths()
files <- all_files[grepl("blank|influent|o3sw", all_files)]

# Chemicals database
db <- StreamFindData::get_ms_tof_spiked_chemicals()
db <- db[grepl("S", db$tag), ]
cols <- c("name", "formula", "mass", "rt", "tag")
db <- db[, cols, with = FALSE]
db
```

<style>
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
</style>

<br>

<br>

***

The StreamFind R package is a data processing workflow designer. Besides data processing, the platform can also be used for data management, visualization and reporting. This guide focuses on describing the general framework behind StreamFind. The StreamFind is centered around [R6](https://r6.r-lib.org/index.html) classes, serving as data processing engines (used as metaphor) for different types of data (e.g. mass spectrometry (MS) and Raman spectroscopy data).  

# Data processing engines

Data processing engines are fundamentally reference classes with methods to manage, process, visualize and report data within a project. The `CoreEngine` is the parent class of all other data specific engines (e.g. `MassSpecEngine` and `RamanEngine`). As parent, the `CoreEngine` manages the project information via the class `ProjectHeaders`, registers the audit trail, and contains uniform functions across child data dedicated engines (e.g. adding and removing analyses from the project).  

```{r}
core <- CoreEngine$new()

core
```

Note that when an empty `CoreEngine` is created, required `ProjectHeaders` are created with name, author, path and date. Yet, `ProjectHeaders` can be specified directly during creation of the `CoreEngine` via the argument `headers` or added to the engine as shown in \@ref(project-headers). The `CoreEngine` does not directly handle data processing. Processing methods are data specific and therefore, are used via the data dedicated engines. Yet, the framework to manage the data processing workflow and the results are implemented in the `CoreEngine` and are therefore, harmonized across engines.  

## Project headers

The `ProjectHeaders` S3 class is meant to hold project information that can be used to identify the engine when for example, combining different engines, or to add extra information, such as description, location, etc. The user can add any kind of attribute but it must have length one and be named. Below, `ProjectHeaders` are created and added to the `CoreEngine`.  

```{r}
headers <- ProjectHeaders(
  name = "Project Example", 
  author = "Person Name", 
  description = "Example of project headers"
)

core$add_headers(headers)

core$get_headers()
```

## ProcessingSettings

Data processing workflows in StreamFind are assembled by combining different processing methods in a specific order. Each processing method uses a specific algorithm for processing/transforming data at a given stage of the workflow. Thus, to harmonize the diversity of processing methods and algorithms available, a general `ProcessingSettings` is used (shown below). This way, the `ProcessingSettings` objects are use as instructions to assemble a data processing workflow within an engine.  

```{r}
ProcessingSettings()
```

A `ProcessingSettings` object must always have the engine type, the call name of the processing method, the name of the algorithm to be used, the origin software, the main developer name and contact as well as a link to further information and the DOI, when available. Lastly but not least, the parameters which is a flexible list of conditions to apply the algorithm during data processing. As example, `ProcessingSettings` for centroiding MS data using the qCentroids from the [qAlgorihtms](https://github.com/odea-project/qAlgorithms) and for annotating features using a native algorithm from StreamFind are shown below. Each `ProcessingSettings` object has a dedicated constructor method with documentation to support the usage. Help pages for processing methods can be obtained with the native R function `?` or `help()`.  

```{r}
MassSpecSettings_CentroidSpectra_qCentroids()
```

```{r}
MassSpecSettings_AnnotateFeatures_StreamFind()
```

## Saving and loading

The `CoreEngine` also holds the functionality to save the project in the engine (as a SQLite file) and load it back. The `save()` method saves the project and the `load()` method loads it, as shown below.  

```{r}
project_file_path <- file.path(getwd(), "/project.sqlite")
core$save(project_file_path)
```

```{r}
file.exists(project_file_path)
```

```{r}
new_core <- CoreEngine$new()
new_core$load(project_file_path)
```

```{r}
# The headers are has the core object although a new_core object was created with default headers
new_core$get_headers()
```

```{r include=FALSE}
file.remove(project_file_path)
```

# Data specific engines

As above mentioned, the `CoreEngine` does not handle data processing directly. The data processing is delegated to child engines. A simple example is given below by creating a child `RamanEngine` and accessing the spectra from the analyses (added as full paths to *.asc* files on disk). Note that the workflow and results are still empty, as no data processing methods were applied.  

```{r}
# Example raman .asc files
raman_ex_files <- StreamFindData::get_raman_file_paths()
raman <- RamanEngine$new(raman_ex_files)
raman
```

```{r}
# setting interactive to TRUE, plotly is used for an interactive plot
raman$plot_spectra(interactive = FALSE)
```

# Editing analyses set 

For data processing, the analysis replicate names and the correspondent blank analysis replicates can be assigned with dedicated methods, as shown below. For instance, the replicate names are used for averaging the spectra in correspondent analyses and the assigned blanks are used for background subtraction, as shown below in \@ref(data-processing).  

```{r}
raman$add_replicate_names(c(rep("Sample", 11), rep("Blank", 11)))
raman$add_blank_names(rep("Blank", 22))
```

```{r}
# The replicate names are modified and the blanks are assigned
raman
```

```{r}
raman$plot_spectra(interactive = FALSE, colorBy = "replicates")
```

# Data processing workflow

As above mentioned, `ProcessingSettings` are used to design an ordered data processing workflow. Below we create a simple 
list of `ProcessingSettings` for processing the Raman spectra.

```{r}
ps <- list(
  # Averages the spectra for each analysis replicate
  RamanSettings_AverageSpectra_StreamFind(),
  
  # Simple normalization based on maximum intensity
  RamanSettings_NormalizeSpectra_minmax(),
  
  # Background subtraction
  RamanSettings_SubtractBlankSpectra_StreamFind(),
  
  # Applies smoothing based on moving average
  RamanSettings_SmoothSpectra_movingaverage(windowSize = 4),
  
  # Removes a section from the spectra from -40 to 470
  RamanSettings_DeleteSpectraSection_StreamFind(list("shift" = c(-40, 300))),
  
  # Removes a section from the spectra from -40 to 470
  RamanSettings_DeleteSpectraSection_StreamFind(list("shift" = c(2000, 3000))),
 
  # Performs baseline correction 
  RamanSettings_CorrectSpectraBaseline_baseline(method = "als", args = list(lambda = 3, p = 0.06, maxit = 10)),
  
  # Performs again normalization using minimum and maximum
  RamanSettings_NormalizeSpectra_minmax()
)
```

```{r}
# The settings are added to the engine but not yet applied
raman$add_settings(ps)

raman$print_workflow()
```

```{r}
# The data processing workflow is applied
raman$run_workflow()
```

# Results

Once the data processing methods are applied, the results can be accessed with the `get_results()` method.  

```{r}
# The spectra results is added after processing the Raman spectra 
raman

# The structure of the spectra results
str(raman$get_results("spectra"))
```

```{r}
# Averaged spectra can be obtained with the dedicated field
raman$spectra
```

```{r}
# Modified spectra which results in a single spectrum
raman$plot_spectra()
```

# Conclusion

This quick guide introduced the general framework behind StreamFind. The StreamFind is a data processing workflow designer that uses R6 classes to manage, process, visualize and report data within a project. The `CoreEngine` is the parent class of all other data specific engines and manages the project information via the class `ProjectHeaders`. The `ProcessingSettings` are used to harmonize the diversity of processing methods and algorithms available. The data processing is delegated to child engines, such as the `RamanEngine` and `MassSpecEngine`. The data processing workflow is assembled by combining different processing methods in a specific order. The results can be accessed with the `get_results()` method or dedicated methods (e.g. `spectra` and `plot_spectra`). StreamFind can also be used via the embedded shiny app for a graphical user interface. See the [StreamFind App Guide](https://odea-project.github.io/StreamFind/articles/app_guide.html) for more information.

***
