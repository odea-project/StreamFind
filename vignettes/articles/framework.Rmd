---
title: 'StreamFind General Introduction'
author:
  name: Ricardo Cunha
  email: cunha@iuta.de
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  bookdown::html_document2:
    fig_caption: true
    toc: true
    number_sections: true
    toc_float: true
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{StreamFind General Introduction}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
# Libraries
library(knitr)
library(kableExtra)
library(magrittr)
library(ggplot2)
library(plotly)
library(StreamFind)

# Global options
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", fig.width = 9, results = "markup", comment = '', message = FALSE, warning = FALSE)

# Working directory
path_wd <- getwd()

```

```{r resources, include=FALSE}
# MS files
all_files <- StreamFindData::get_ms_file_paths()
files <- all_files[grepl("blank|influent|o3sw", all_files)]

# Chemicals database
db <- StreamFindData::get_ms_tof_spiked_chemicals()
db <- db[grepl("S", db$tag), ]
cols <- c("name", "formula", "mass", "rt", "tag")
db <- db[, cols, with = FALSE]
db
```

<style>
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
</style>

<br>

<br>

***

# Platform

The StreamFind platform is a data processing workflow designer with a microservices architecture.
Besides data processing, the platform can also be used for data management, visualization and reporting.
This guide focuses on describing the general framework behind the StreamFind, using the already available R library as example.
Development of the StreamFind platform as Python and C++ libraries is planned and will support other types of data.
The behavior and usage of the StreanFind is expected to be similar across R, Python and C++.
This is achieved by using reference classes with harmonized methods for the different languages.
For instance, the StreamFind R library is centered around [R6](https://r6.r-lib.org/index.html) classes, serving as data 
processing engines (used as metaphor) for different types of data (e.g. mass spectrometry (MS) and Raman spectroscopy data).  

# Data processing engines

Data processing engines are fundamentally reference classes with methods to manage, process, visualize and report data within a project.
The `CoreEngine` is the parent class of all other data specific engines.
As parent, the `CoreEngine` manages the project information via the class `ProjectHeaders` and registers the audit trail as well as other uniform functions (e.g. adding and removing analyses from the project) across child data dedicated engines.  

```{r}
core <- CoreEngine$new()

core
```

Note that when an empty `CoreEngine` is created, required `ProjectHeaders` are created with name, author, path and date.
Yet, `ProjectHeaders` can be specified directly during creation of the `CoreEngine` via the argument `headers` or added to the engine as shown in \@ref(project-headers).
The `CoreEngine` does not directly handles data processing.
Processing methods are data specific and therefore, are used via the data dedicated engines (e.g., `MassSpecEngine` and `RamanEngine`). 
Yet, the framework to manage the data processing workflow and the results are implemented in the `CoreEngine` 
and are therefore, harmonized across engines.  

## Project headers

The `ProjectHeaders` class is meant to hold project information that can be used to identify the engine when for example, 
combining different engines, or to add extra information, such as description, location, etc.
The user can add any kind of attribute but it must have length one and be named.
Below, `ProjectHeaders` are created and added to the `CoreEngine`.  

```{r}
headers <- ProjectHeaders(
  name = "Project Example", 
  author = "Person Name", 
  description = "Example of project headers"
)

core$add_headers(headers)

core$get_headers()
```

## ProcessingSettings

The unit of a data processing workflow in a data dedicated engine is a data processing method, which uses a specific algorithm for data processing.
To harmonize the diversity of processing methods and algorithms available, a general `ProcessingSettings` is used (shown below).
This way, the `ProcessingSettings` objects are use as instructions to assemble a data processing workflow within the engine.  

```{r}
ProcessingSettings()
```

A `ProcessingSettings` object must always have the call name of the processing method, the name and version of the algorithm to be applied, the origin software, the main developer name and contact as well as a link to further information and the DOI, when available.
Lastly but not least, the parameters which is a flexible list of conditions to apply the algorithm during data processing.
As example, `ProcessingSettings` for centroiding MS data using the qCentroids from the [qAlgorihtms](https://github.com/odea-project/qAlgorithms) and for annotating features using a native algorithm from StreamFind R library are shown below.
Each `ProcessingSettings` object has a dedicated constructor method with documentation to support the usage.  

```{r}
Settings_centroid_spectra_qCentroids()
```

```{r}
Settings_annotate_features_StreamFind()
```

## Analysis

The actual data for a given engine is handled via dedicated analysis class objects.
Similar to the engines, these have the class `Analysis` as parent class to handle common attributes and methods.
For instance, name, replicate and blank are example of common attributes across different data types.  

```{r}
Analysis()
```

A child example is given below for the `MassSpecAnalysis` class to be used in the `MassSpecEngine`.
The method `parse_MassSpecAnalysis` uses a vector with file paths as argument, returning a list of `MassSpecAnalysis` object for each file path.
Extra attributes are added to further describe the specific analysis.
The structure and complexity of the class is therefore dependent on the type of data.  

```{r, message=FALSE}
ms_file <- StreamFindData::get_ms_file_paths()[1]
basename(ms_file)
```

```{r}
# Example for Mass Spectrometry analysis
StreamFind:::rcpp_parse_ms_analysis(ms_file)
```


# Child engines

As above mentioned, the `CoreEngine` does not handle data processing directly. The data processing is delegated to child 
engines. A simple example is given below for creating a child `RamanEngine` and accessing the spectra from the analyses 
added as *.asc* files. Note that workflow and results are still empty, as no data processing methods were applied.  

```{r}
# Example raman .asc files
raman_ex_files <- StreamFindData::get_raman_file_paths()
raman <- RamanEngine$new(raman_ex_files)
raman
```

```{r}
# setting interactive to TRUE, plotly is used for an interactive plot
raman$plot_spectra(interactive = FALSE)
```

# Editing analyses set 

For data processing, the analyses replicate names and the correspondent blank analysis replicates can be assigned with 
dedicated methods, as shown below. For instance, the replicate names are used for averaging the spectra in correspondent 
analyses and the assigned blanks are used for background subtraction, as shown below in \@ref(data-processing).  

```{r}
raman$add_replicate_names(c(rep("Sample", 11), rep("Blank", 11)))
raman$add_blank_names(rep("Blank", 22))
```

```{r}
# The replicate names are modified and the blanks are assigned
raman
```

# Data processing

As above mentioned, `ProcessingSettings` are used to design the data processing workflow. Below we create a simple 
list of `ProcessingSettings` for processing the Raman spectra.

```{r}
ps <- list(
  # Averages the spectra for each analysis replicate
  Settings_average_spectra_StreamFind(),
  
  # Simple normalization based on maximum intensity
  Settings_normalize_spectra_minmax(),
  
  # Background subtraction
  Settings_subtract_blank_spectra_StreamFind(),
  
  # Applies smoothing based on moving average
  Settings_smooth_spectra_movingaverage(windowSize = 4),
  
  # Removes a section from the spectra from -40 to 470
  Settings_delete_spectra_section_StreamFind(list("shift" = c(-40, 300))),
  
  # Removes a section from the spectra from -40 to 470
  Settings_delete_spectra_section_StreamFind(list("shift" = c(2000, 3000))),
 
  # Performs baseline correction 
  Settings_correct_spectra_baseline_baseline(method = "als", args = list(lambda = 3, p = 0.06, maxit = 10)),
  
  # Performs again normalization using mixmax
  Settings_normalize_spectra_minmax()
)
```

```{r}
# The settings are added to the engine but not yet applied
raman$add_settings(ps)

raman$print_workflow()
```

```{r}
# The data processing workflow is applied
raman$run_workflow()
```

# Results

Once the data processing methods are applied, the results can be accessed with the `get_results()` method.  

```{r}
# The spectra data module is added after processing the Raman spectra 
raman$print_results()

# The structure of the spectra results
str(raman$get_results("spectra"))
```

```{r}
# Averaged spectra can be obtained with the dedicated field
raman$spectra
```

```{r}
# Modified spectra which results in a single spectrum
raman$plot_spectra()
```

***
