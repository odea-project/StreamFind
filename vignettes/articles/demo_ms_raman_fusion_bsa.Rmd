---
title: 'Fusion of Mass Spectrometry and Raman Spectroscopy Data'
subtitle: |
  | Comparison of different bovine serum albumin formulations
  | <br>
author:
  name: Ricardo Cunha
  email: cunha@iuta.de
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  bookdown::html_document2:
    fig_caption: true
    toc: true
    number_sections: true
    toc_float: true
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{Fusion of Mass Spectrometry and Raman Spectroscopy Data}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(knitr)
library(kableExtra)
library(magrittr)
library(ggplot2)
library(plotly)
library(data.table)
library(StreamFind)

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.align = "center", fig.width = 9, results = "asis", comment = "")

main_dir <- "C:/Users/apoli/Documents/iSoft/BSA_products"

all_files <- list.files(main_dir, full.names = TRUE)

ms_files_d <- all_files[grepl(".d", fixed = T, all_files)][1:13]

ms_files <- all_files[grepl(".mzML", fixed = T, all_files)]

raman_dirs <- all_files[grepl("_Raman", fixed = TRUE, all_files)]

raman_files <- character()

for (i in raman_dirs) raman_files <- c(raman_files, list.files(i, full.names = TRUE))

raman_unified_files <- list.files(main_dir, pattern = "asc", full.names = TRUE)
```

<style>
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
</style>

<br>

<br>

***

# Introduction

Mass Spectrometry (MS) and Raman spectroscopy (Raman) are orthogonal techniques. While MS measures the chemical composition of a given sample, Raman is able to provide structural information of the constituents. This orthogonality is interesting when applying quality evaluation workflows, where both chemical composition and structural integrity are relevant. In this article, we show how MS and Raman can be pre-processed and fused for a combined statistical evaluation to support quality control. As test sets, we use four different bovine serum albumin (BSA) products.

# Pre-processing

The preliminary processing is applied differently for MS and Raman as the raw data structure does not allow direct fusion of the data. Therefore, in the following two sub-chapters we describe the pre-processing workflow steps for MS and Raman data.

## MS

The four different BSA products were measured in triplicate in full scan MS mode. Giving the following set of *.d* files.

```{r}
# Raw data from an Agilent QTOF
basename(ms_files_d)
```

The first step is conversion to open source *mzML* format using *MSConvert* from [ProteoWizard](https://proteowizard.sourceforge.io/index.html) CLI, as shwon below.

```{r eval=FALSE}
# Add option to centroid MS1 data before conversion
optList <- list(filter = "peakPicking vendor msLevel=1")

# convert_ms_files(ms_files_d, outputFormat = "mzML", optList = optList)
```

After conversion, the *mzML* files are created in the same directory. The converted files are listed below.

```{r}
basename(ms_files)
```

With the *mzML* files, a new *MassSpecEngine* is created to start the MS pre-processing project. Below the engine is created and the analysis replicate names are modified to properly assign the analyses within each replicate measurement.

```{r}
ms <- MassSpecEngine$new(files = ms_files, headers = list(name = "BSA Quality Evaluation Project"))

# Changes the analysis replicate names
ms$add_replicate_names(sub("_\\d+$", "", ms$get_analysis_names()))

# Overview
ms
```

```{r, fig.cap="Total ion chromatograms for each analysis replicate."}
ms$plot_tic(colorBy = "replicates")
```

The data processing workflow is based on applied processing methods within the engine. As pre-processing, the following steps are applied:

1. Find the retention time elution of BSA based on the total ion chromatogram (TIC) of each analysis; 
2. Find the mass-to-charge (*m/z*) range of BSA for deconvolution of charges; 
3. Deconvolute charges for estimation of the exact mass, in DA; 

The individual processing steps are applied according to *ProcessingSettings*. Below, the integration of the TIC is shown using the dedicated *ProcessingSettings* after loading the TIC chromatograms from the analysis files and smoothing using a moving average approach.

```{r}
ms$load_chromatograms("TIC")
```

```{r}
# Smoothing of the TIC chromatogram for improving peak finding
ms$smooth_chromatograms(Settings_smooth_chromatograms_movingaverage(windowSize = 5))
```

```{r}
# Settings for integration of the TIC
ps_ic <- Settings_integrate_chromatograms_StreamFind(
  merge = TRUE,
  closeByThreshold = 5,
  minPeakHeight = 3E6,
  minPeakDistance = 3,
  minPeakWidth = 20,
  maxPeakWidth = 200,
  minSN = 5
)

ps_ic
```

```{r}
ms$integrate_chromatograms(ps_ic)
```

```{r ,fig.cap="TIC with integrated peaks."}
ms$plot_chromatograms_peaks(colorBy = "replicates")
```

The average retention time of the main peak was `r mean(ms$chromatograms_peaks$rt)` seconds with a standard deviation of `r sd(ms$chromatograms_peaks$rt)`. The following step is to estimate the *m/z* range of BSA for deconvolution of charges. For this, the MS1 spectra of the peak retention time +/- 0.1 seconds is plotted.

```{r, fig.cap="Spectra of BSA for each analysis replicate at the peak maximum."}
ms$plot_ms1(rt = mean(rbindlist(ms$chromatograms_peaks)$rt), sec = 0.5,
  colorBy = "replicates", presence = 0.1, minIntensity = 1000, interactive = FALSE
)
```

According to the MS1 spectra, the *m/z* from 1200 to 2000 can be added to the dedicated *ProcessingSettings* for deconvolution, as shown below. The first step is to load from the mzML files the traces within the retention time and *m/z* ranges selected.

```{r}
av_rt <- mean(rbindlist(ms$chromatograms_peaks)$rt)

spectra_ranges <- data.table("mzmin" = 1200, "mzmax" = 2000, "rtmin" = av_rt - 1, "rtmax" = av_rt + 1)

# ms <- MassSpecEngine$new(files = ms_files, headers = list(name = "BSA Quality Evaluation Project"))
# 
# ms$add_replicate_names(sub("_\\d+$", "", ms$get_analysis_names()))

ms$load_spectra(mz = spectra_ranges)

# ms$cluster_spectra(Settings_cluster_spectra_StreamFind(val = "mz", clustVal = 0.01, presence = 0.2))
# ms$calculate_spectra_charges(Settings_calculate_spectra_charges_StreamFind(roundVal = 15, relLowCut = 0.2, absLowCut = 8000))
# ms$deconvolute_spectra(Settings_deconvolute_spectra_StreamFind())
# ms$smooth_spectra(Settings_smooth_spectra_movingaverage(windowSize = 15))
# ms$normalize_spectra(Settings_normalize_spectra_minmax()) #liftTozero = TRUE
# ms$normalize_spectra(Settings_normalize_spectra_blockweight())
# ms$bin_spectra(Settings_bin_spectra_StreamFind(bins = list("rt" = 2, "mass" = 25), refBinAnalysis = 1))

# View(ms$spectra)
# ms$plot_spectra()
# ms$spectra_charges
# ms$plot_spectra_charges()
# ms$correct_spectra_baseline(Settings_correct_spectra_baseline_baseline())
# ms$plot_deconvoluted_spectra()
# ms$plot_spectra(interactive = T)
# ms$plot_spectra_charges()
# ms$spectra_charges
# ms$deconvoluted_spectra
# ms$spectra_charges
# ms$plot_spectra_charges()
# ms$get_spectra()
# ms$spectra
# ms$has_results("spectra")
# ms$spectra
# ms$plot_spectra(interactive = T)
# View(ms$get_results("spectra"))
```

Below we create an ordered list of *ProcessingSettings* objects for each processing method to be applied and add the list to the engine. The exact parameters for each method can be optimized individually by calling the specific method directly. Once the settings are added, the workflow can be inspected with the `print_workflow()` method.

```{r}
ms_workflow_settings <- list(
  Settings_cluster_spectra_StreamFind(val = "mz", clustVal = 0.01, presence = 0.2),
  Settings_calculate_spectra_charges_StreamFind(roundVal = 15, relLowCut = 0.2, absLowCut = 8000),
  Settings_deconvolute_spectra_StreamFind(),
  Settings_smooth_spectra_movingaverage(windowSize = 15),
  Settings_bin_spectra_StreamFind(bins = list("rt" = 2, "mass" = 25), refBinAnalysis = 1),
  Settings_average_spectra_StreamFind(),
  Settings_normalize_spectra_minmax(),
  Settings_normalize_spectra_blockweight()
)

# Remove the settings added to integrate the TIC chromatograms
ms$remove_settings()

ms$add_settings(ms_workflow_settings)

ms$print_workflow()
```

The `run_workflow()` method is called to run the data processing. The workflow will be applied in the order the methods were added to the engine.

```{r}
ms$run_workflow()
```

```{r ,fig.cap="Deconvoluted and pre-processed spectra for each analysis."}
ms$plot_spectra(colorBy = "analyses")
```

## Raman

The four different BSA products were measured using an online coupling of size exclusion chromatography (SEC) with capillary-enhanced Raman spectroscopy (CERS) through a liquid-core optical fibre flow cell, as reported by Thissen et al. (DOI: 10.1021/acs.analchem.3c03991). The raw spectra was converted to *asc* format for each retention time. A subset of the background and the main BSA elution peak was selected for pre-processing. The first pre-processing step is to merge the individual *asc* files into a main file for each BSA product, as shown below.

```{r eval=FALSE}
# Engine with all asc files for each spectrum and each BSA product
#raman <- RamanEngine$new(raman_files, runParallel = FALSE)

# Folder names of each BSA product
#base_dirs <- basename(dirname(raman_files))

# The folder name is used as replicate name, which is also used as file name for the unified data file
#raman$add_replicate_names(base_dirs)

# The spectra are merged into a time series for each BSA product
#raman$merge_spectra_time_series()
```

A new engine is created with the unified file for each BSA product and then plotted as a time series.

```{r}
raman <- RamanEngine$new(raman_unified_files, runParallel = FALSE)

# Modifying the replicate names to match the MS analyses
raman$add_replicate_names(unique(ms$get_replicate_names())[1:4])

raman
```

```{r ,fig.cap="Time series chromatogram for each BSA product. Each data point is a Raman spectrum at a specific retention time."}
raman$plot_chromatograms(yLab = "Abbundance sum for each spectrum", colorBy = "analyses")
```

```{r ,fig.cap="Raw averaged Raman spectra for each BSA product between minute 5.5 and 8."}
raman$plot_spectra(rt = c(5.5, 8), colorBy = "analyses")
```

As for MS data, the workflow for Raman data is also assembled using an ordered list of processing settings, as shown below. 

```{r}
raman_workflow_settings <- list(
  Settings_bin_spectra_StreamFind(unitsVal = "rt", unitsNumber = 3),
  Settings_normalize_spectra_minmax(),
  Settings_subtract_spectra_section_StreamFind(sectionWindow = c(0, 3)),
  Settings_smooth_spectra_savgol(fl = 21, forder = 4, dorder = 0),
  Settings_delete_spectra_section_StreamFind(list("shift" = c(-100, 330))),
  Settings_delete_spectra_section_StreamFind(list("shift" = c(1800, 2600))),
  Settings_correct_spectra_baseline_airpls(lambda = 45, differences = 1, itermax = 30),
  Settings_delete_spectra_section_StreamFind(list("rt" = c(0, 5.5))),
  Settings_delete_spectra_section_StreamFind(list("rt" = c(8, 11))),
  Settings_average_spectra_StreamFind(collapseTime = TRUE),
  Settings_normalize_spectra_minmax(),
  Settings_normalize_spectra_blockweight()
)

raman$add_settings(raman_workflow_settings)

raman$print_workflow()
```

A particularity of the workflow for Raman data is that the background subtraction (workflow step number 3) is performed from a time region (i.e., between 0 and 3 minutes) before the main BSA peak, which is from approximately from 5 to 8 minutes. The workflow is then applied using the `run_workflow()` method.

```{r}
raman$run_workflow()
```

```{r ,fig.cap="Processed Raman spectra for each BSA product."}
raman$plot_spectra()
```

# Fusion

The fusion of MS and Raman data is performed by combining the processed data from both engines. The data is then merged into a single data frame for statistical evaluation using a MachineLearningEngine, which is under development.

```{r}
rSpec <- raman$get_spectra()
mrSpec <- matrix(rSpec$intensity, nrow = length(unique(rSpec$replicate)), ncol = length(unique(rSpec$shift)), byrow = TRUE, dimnames = list(as.character(unique(rSpec$replicate)), as.character(unique(rSpec$shift))))

mSpec <- ms$get_spectra()
mmSpec <- matrix(mSpec$intensity, nrow = length(unique(mSpec$replicate)), ncol = length(unique(mSpec$bins)), byrow = TRUE, dimnames = list(unique(mSpec$replicate), unique(mSpec$bins)))

fusedMat <- cbind(mmSpec, mrSpec)
```

```{r}
cl <- StreamFind:::.get_colors(rownames(fusedMat))
fig <- plot_ly()
xVal <- seq_len(length(fusedMat[1, ]))
for (i in seq_len(nrow(fusedMat))) {
  fig <- fig %>% add_trace(
    x = xVal,
    y = fusedMat[i, ],
    type = "scatter", mode = "lines",
    line = list(width = 0.5, color = unname(cl[i])),
    name = names(cl)[i],
    legendgroup = names(cl)[i],
    showlegend = TRUE
  )
}
xaxis <- list(
  linecolor = toRGB("black"),
  linewidth = 2, title = "Bins",
  titlefont = list(size = 12, color = "black")
)
yaxis <- list(
  linecolor = toRGB("black"),
  linewidth = 2, title = "Intensity",
  titlefont = list(size = 12, color = "black")
)
fig <- fig %>% plotly::layout(xaxis = xaxis, yaxis = yaxis)
fig
```

# Statistic analysis

Ideas:

MCR-ALS (Multivariate Curve Resolution - Alternating Least Squares)


## PCA

```{r ,echo=FALSE,fig.width=10,fig.height=5}
data_pca <- prcomp(fusedMat, center = TRUE, scale. = FALSE)
pcs <- data_pca$x
pc_data <- data.frame(PC1 = pcs[, 1], PC2 = pcs[, 2], Species = rownames(pcs))
pca_plot <- plot_ly(
  data = pc_data,
  x = ~PC1,
  y = ~PC2,
  type = "scatter",
  mode = "markers+text",
  text = ~Species,
  textfont = list(size = 14),
  marker = list(size = 12),
  textposition = "top center"
) %>%
  layout(
    title = "PCA Plot with Annotations",
    xaxis = list(title = "Principal Component 1"),
    yaxis = list(title = "Principal Component 2")
  )
pca_plot
```

```{r ,echo=FALSE,fig.width=10,fig.height=4}
loadings <- as.data.frame(data_pca$rotation[, 1:2])

loadings_plot <- plot_ly()

loadings_plot <- loadings_plot %in% add_trace(loadings[rownames(loadings) %in% colnames(mrSpec), ],
  x = loadings[rownames(loadings) %in% colnames(mrSpec), "PC1"],
  y = loadings[rownames(loadings) %in% colnames(mrSpec), "PC2"],
  type = "scatter",
  mode = "markers",
  # text = rownames(loadings)[(rownames(loadings) %in% colnames(mrSpec))],
  # textfont = list(size = 14),
  marker = list(size = 10)
)
 
loadings_plot <- loadings_plot %in% add_trace(
  data = loadings[rownames(loadings) %in% colnames(mmSpec), ],
  x = ~PC1,
  y = ~PC2,
  type = "scatter",
  mode = "markers+text",
  text = rownames(loadings)[(rownames(loadings) %in% colnames(mmSpec))],
  textfont = list(size = 14),
  marker = list(size = 10, color = "red")
)

loadings_plot %>% layout(
    title = "PCA Loadings Plot",
    xaxis = list(title = "Principal Component 1"),
    yaxis = list(title = "Principal Component 2")
  )

```

## Correlation Analysis

```{r}
# Calculate correlation matrix
correlation_matrix <- cor(fusedMat)

# Print correlation matrix
print(correlation_matrix)
```

## Distance Metrics

```{r}
# Calculate Euclidean distance matrix
distance_matrix <- dist(fusedMat)

# Print distance matrix
print(distance_matrix)
```

## Hierarchical Clustering

```{r}
# Perform hierarchical clustering
hc_result <- hclust(dist(fusedMat))

# Plot dendrogram
plot(hc_result)
```

## Multidimensional Scaling (MDS)

```{r}
# Calculate Euclidean distance matrix
distance_matrix <- dist(fusedMat)

# Perform MDS
mds_result <- cmdscale(distance_matrix)

# Plot MDS results
plot(mds_result[,1], mds_result[,2], pch = 19, col = "blue", xlab = "MDS1", ylab = "MDS2")

```

## Kernel Methods

```{r}
# Example code for kernel PCA (assuming Gaussian kernel)
library(kernlab)
kpca_result <- kpca(fusedMat, kernel = "rbfdot")

# Plot kernel PCA results
plot(kpca_result@pcv[,1], kpca_result@pcv[,2], pch = 19, col = "blue", xlab = "KPCA1", ylab = "KPCA2")
```

## Feature Selection and Importance

```{r}
# Example code for feature selection (using random forest as an example)
library(randomForest)
rf_model <- randomForest(fusedMat, importance = TRUE)
print(importance(rf_model))

var_importance <- importance(rf_model)
var_names <- rownames(var_importance)
var_scores <- var_importance[,"MeanDecreaseAccuracy"]  # Adjust column name as needed

# Sort variables by importance score
sorted_indices <- order(var_scores, decreasing = TRUE)
sorted_var_names <- var_names[sorted_indices]
sorted_var_scores <- var_scores[sorted_indices]

# Create bar plot
barplot(sorted_var_scores, names.arg = sorted_var_names, las = 2, cex.names = 0.8,
        main = "Variable Importance", xlab = "Variables", ylab = "Importance Score")
```


##  Cosine similarity and k-means clustering

```{r}
vectors <- fusedMat

similarity_matrix <- tcrossprod(vectors) / (sqrt(rowSums(vectors^2)) %*% t(sqrt(rowSums(vectors^2))))

# Perform k-means clustering
k <- 3  # Number of clusters
clusters <- kmeans(similarity_matrix, centers = k)

# Print cluster assignments
print(clusters$cluster)
```

## Similarity Measures

```{r}
vectors <- fusedMat

# Calculate cosine similarity between pairs of vectors
cosine_similarity <- function(x, y) {
  return(sum(x * y) / (sqrt(sum(x^2)) * sqrt(sum(y^2))))
}

similarity_matrix <- matrix(NA, nrow = nrow(vectors), ncol = nrow(vectors))
for (i in 1:nrow(vectors)) {
  for (j in 1:nrow(vectors)) {
    similarity_matrix[i, j] <- cosine_similarity(vectors[i,], vectors[j,])
  }
}

# Print similarity matrix
print(similarity_matrix)
```

## PCA 2

```{r}
vectors <- fusedMat

# Perform PCA
pca_result <- prcomp(vectors)

# Plot PCA results
plot(pca_result$x[,1], pca_result$x[,2], pch = 19, col = "blue", xlab = "PC1", ylab = "PC2")
```

## k-nearest neighbors (KNN) classification

```{r}
training_data <- fusedMat[c(1, 4), ]  # Replace this with your actual training data
test_data <- fusedMat[2:3, ]  # Replace this with your actual test data
labels <- rownames(fusedMat[c(1, 4), ])  # Replace this with your actual labels for training data

# Train KNN model
library(class)
knn_model <- knn(train = training_data, test = test_data, cl = labels, k = 3)

# Print predicted labels for test data
print(knn_model)
```

## Partial Least Squares (PLS)

Not applicable!

```{r}
# Example data
X <- fusedMat  # Replace this with your actual predictor variables
Y <- seq_len(nrow(fusedMat))  # Replace this with your actual response variable

# Perform PLS regression
library(pls)
pls_model <- plsr(Y ~ X, ncomp = 2)  # You can adjust the number of components (ncomp) as needed

# Predict using the PLS model
predicted_values <- predict(pls_model, newdata = X)

# Print predicted values
print(predicted_values)
```

## Support Vector Regression (SVR)

Not applicable!

```{r}
# Example data
X <- matrix(rnorm(100), nrow = 10)  # Replace this with your actual predictor variables
Y <- rnorm(10)  # Replace this with your actual response variable

# Train SVR model
library(e1071)
svr_model <- svm(Y ~ ., data = as.data.frame(X), kernel = "radial", epsilon = 0.1, cost = 1)  # Adjust parameters as needed

# Predict using the SVR model
predicted_values <- predict(svr_model, newdata = as.data.frame(X))

# Print predicted values
print(predicted_values)

```

